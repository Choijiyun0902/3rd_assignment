## 수업내용정리

### 1. 지도 학습: 
 - 함수 $f(x)$학습 > 주어진 x로 y를 예측하기 위함
 - 학습샘플과 정답을 함께 제공 Given($x_1, y_1$)

### 2. 회귀(Regression): 연속적인 수치 값을 예측하는 문제

 - Linear Regression: $x$(샘플)를 입력받아 y(정답)을 예측

    $$y = b + w_1x_1 + w_2x_2 + ⋯ + w_nx_n = b + w^Tx$$

    - loss function : 오차를 수치로 나타냄
        - MSE(Mean Squared Error)

            $$\text{MSE} = \frac{1}{N}\displaystyle\sum_{i=1}^N(y_i - (w^Tx_i + b))^2$$
        
        - MAE(Mean Absolute Error)

            $$\text{MAE} = \frac{1}{N} \displaystyle\sum_{i=1}^{N} |y_i - \hat{y}_i|$$

        - Cross Entropy

            $$\text{Cross-Entropy} = -\frac{1}{N} \displaystyle\sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$

 - Polynomial Regression: 다항 회귀로 선형회귀에 포함, 차수가 높으면 과적합(overfitting)의 문제가 있을 수 있음

    $$y = w_0 + w_1x + w_2x^2 + ⋯ + w_dx^d$$

 - Ridge/Lasso Regression: 규제 기법으로 손실함수에 규제를 추가해 오버피팅을 방지
    - Ridge regression = L2

    $$J(w) = \frac{1}{N}\displaystyle\sum_{i=1}^N(y_i - (w^Tx_i + b))^2 + λ\displaystyle\sum_{j=1}^n w_j^2$$

    - Lasso regression = L1

    $$J(w) = \frac{1}{N}\displaystyle\sum_{i=1}^N(y_i - (w^Tx_i + b))^2 + λ\displaystyle\sum_{j=1}^n |w|$$

### 3. 분류(Classification): 클래스를 예측하는 문제

 - Perceptron: 이진 분류에 사용되는 초기 모델 0, 1을 반환

 - SVM(Support Vector Machine): 두 클래스 사이의 마진을 최대화하여 분류(마진이 커지면 클래스를 더 잘 분류할 수 있음)

 - Logistic Regression: 시그모이드 함수를 사용하여 출력값 반환
   - sigmoid

    	$$σ(z) = \frac{1}{1+e^{-z}}$$
    
 - Softmax Regression: 다중 분류 문제 해결에 사용

    $$P(y = k|x)=\frac{e^{zk}}{\displaystyle\Sigma_{j=1}^K e^{zj}}$$

### 4. 트리 모델(Tree Model): 데이터 특성에 따른 의사결정 규칙을 통해 분류 또는 회귀

 - Decision Tree Classifier: 분류에 활용, Gini, Entropy 불순도를 활용

    $$\text{Gini}(D) = 1 - \displaystyle\sum_{i=1}^C P_i^2$$
 
p_i: 클래스 i에 속할 확률, C : 클래스 개수 

 - Decision Tree Regressor: 회귀, MSE 지표로 활용

 - 앙상블: Overfitting을 방지하기 위해 랜덤 포레스트(Random Forest)나 그레디언트 부스팅(Gradient Boosting)과 같은 기법을 사용